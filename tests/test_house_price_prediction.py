"""
ðŸ§ª Test Suite for House Price Prediction Project
Comprehensive unit tests for all modules

Author: Data Science Team
Version: 2.0
"""

import pytest
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import sys
import os

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.preprocess import (
    load_data, preprocess_data, handle_missing_values, 
    encode_categorical, scale_features, explore_data
)
from src.train_models import train_models, get_best_model
from src.evaluate import evaluate_model, cross_validate_model
from utils.helpers import setup_logger, validate_data_integrity

class TestDataPreprocessing:
    """Test suite for data preprocessing functions"""
    
    @pytest.fixture
    def sample_data(self):
        """Create sample data for testing"""
        np.random.seed(42)
        return pd.DataFrame({
            'GrLivArea': np.random.randint(1000, 3000, 100),
            'OverallQual': np.random.randint(1, 11, 100),
            'YearBuilt': np.random.randint(1950, 2020, 100),
            'Neighborhood': np.random.choice(['NAmes', 'CollgCr', 'OldTown'], 100),
            'SalePrice': np.random.randint(100000, 500000, 100),
            'MissingCol': [None if i % 10 == 0 else 'value' for i in range(100)]
        })
    
    def test_load_data_structure(self, sample_data):
        """Test data loading returns correct structure"""
        # Save sample data temporarily
        test_path = PROJECT_ROOT / "tests" / "temp_data.csv"
        sample_data.to_csv(test_path, index=False)
        
        try:
            loaded_data = load_data(str(test_path))
            assert isinstance(loaded_data, pd.DataFrame)
            assert len(loaded_data) == 100
            assert 'SalePrice' in loaded_data.columns
        finally:
            if test_path.exists():
                test_path.unlink()
    
    def test_handle_missing_values(self, sample_data):
        """Test missing value handling"""
        processed_data = handle_missing_values(sample_data.copy())
        
        # Should have fewer or equal missing values
        original_missing = sample_data.isnull().sum().sum()
        processed_missing = processed_data.isnull().sum().sum()
        assert processed_missing <= original_missing
    
    def test_encode_categorical(self, sample_data):
        """Test categorical encoding"""
        encoded_data = encode_categorical(sample_data.copy())
        
        # Check if categorical columns are properly encoded
        assert 'Neighborhood' in encoded_data.columns or any('Neighborhood_' in col for col in encoded_data.columns)
    
    def test_scale_features(self, sample_data):
        """Test feature scaling"""
        # Remove non-numeric columns for scaling test
        numeric_data = sample_data.select_dtypes(include=[np.number])
        scaled_data, scaler = scale_features(numeric_data.copy())
        
        # Check if scaling worked
        assert abs(scaled_data['GrLivArea'].mean()) < 1e-10  # Should be close to 0
        assert abs(scaled_data['GrLivArea'].std() - 1) < 1e-10  # Should be close to 1
    
    def test_preprocess_data_pipeline(self, sample_data):
        """Test complete preprocessing pipeline"""
        try:
            X_train, X_test, y_train, y_test, preprocessor, feature_names = preprocess_data(
                sample_data.copy(), test_size=0.2, random_state=42
            )
            
            # Check output shapes
            assert len(X_train) == 80  # 80% of 100
            assert len(X_test) == 20   # 20% of 100
            assert len(y_train) == 80
            assert len(y_test) == 20
            
            # Check no missing values in processed data
            assert not pd.DataFrame(X_train).isnull().any().any()
            assert not pd.DataFrame(X_test).isnull().any().any()
            
            # Check feature names
            assert isinstance(feature_names, list)
            assert len(feature_names) > 0
            
        except Exception as e:
            pytest.skip(f"Preprocessing test requires additional setup: {e}")

class TestModelTraining:
    """Test suite for model training functions"""
    
    @pytest.fixture
    def sample_training_data(self):
        """Create sample training data"""
        np.random.seed(42)
        X = np.random.rand(100, 5)
        y = np.random.rand(100) * 100000 + 100000
        return X, y
    
    def test_model_training_basic(self, sample_training_data):
        """Test basic model training functionality"""
        X, y = sample_training_data
        X_train, X_test = X[:80], X[80:]
        y_train, y_test = y[:80], y[80:]
        
        try:
            simple_results, detailed_results = train_models(
                X_train, X_test, y_train, y_test, tune_hyperparams=False
            )
            
            # Check results structure
            assert isinstance(simple_results, dict)
            assert isinstance(detailed_results, dict)
            assert len(simple_results) > 0
            
            # Check required metrics
            for model_name, metrics in simple_results.items():
                assert 'R2' in metrics
                assert 'RMSE' in metrics
                assert 'MAE' in metrics
                
        except Exception as e:
            pytest.skip(f"Model training test requires additional setup: {e}")
    
    def test_get_best_model(self):
        """Test best model selection"""
        sample_results = {
            'Model1': {'model': 'dummy1', 'R2': 0.8, 'RMSE': 25000},
            'Model2': {'model': 'dummy2', 'R2': 0.9, 'RMSE': 20000},
            'Model3': {'model': 'dummy3', 'R2': 0.85, 'RMSE': 22000}
        }
        
        best_name, best_model = get_best_model(sample_results)
        assert best_name == 'Model2'  # Highest R2
        assert best_model == 'dummy2'

class TestModelEvaluation:
    """Test suite for model evaluation functions"""
    
    @pytest.fixture
    def mock_model(self):
        """Create a mock model for testing"""
        class MockModel:
            def predict(self, X):
                return np.random.rand(len(X)) * 100000 + 100000
        return MockModel()
    
    def test_evaluate_model(self, mock_model):
        """Test model evaluation function"""
        X_test = np.random.rand(20, 5)
        y_test = np.random.rand(20) * 100000 + 100000
        
        try:
            metrics = evaluate_model(mock_model, X_test, y_test, model_name="Test")
            
            # Check metrics structure
            assert 'r2' in metrics
            assert 'rmse' in metrics
            assert 'mae' in metrics
            assert isinstance(metrics['r2'], float)
            assert isinstance(metrics['rmse'], float)
            assert isinstance(metrics['mae'], float)
            
        except Exception as e:
            pytest.skip(f"Evaluation test requires additional setup: {e}")

class TestUtilityFunctions:
    """Test suite for utility functions"""
    
    def test_setup_logger(self):
        """Test logger setup"""
        try:
            logger = setup_logger('test_logger', 'INFO')
            assert logger.name == 'test_logger'
            
            # Test logging
            logger.info("Test log message")
            
        except Exception as e:
            pytest.skip(f"Logger test requires additional setup: {e}")
    
    def test_validate_data_integrity(self):
        """Test data validation"""
        # Valid data
        valid_data = pd.DataFrame({
            'col1': [1, 2, 3, 4, 5],
            'col2': ['a', 'b', 'c', 'd', 'e']
        })
        
        try:
            is_valid, issues = validate_data_integrity(valid_data)
            assert isinstance(is_valid, bool)
            assert isinstance(issues, list)
            
        except Exception as e:
            pytest.skip(f"Data validation test requires additional setup: {e}")

class TestStreamlitApp:
    """Test suite for Streamlit application"""
    
    def test_app_imports(self):
        """Test that app.py can be imported without errors"""
        try:
            import app
            assert hasattr(app, 'main')
        except ImportError as e:
            pytest.skip(f"App import test requires Streamlit: {e}")
    
    def test_model_loading(self):
        """Test model loading functionality"""
        try:
            from app import load_model_and_data
            model, preprocessor, feature_names, model_name, performance = load_model_and_data()
            
            # Should return something even if model doesn't exist
            assert model_name is not None
            
        except Exception as e:
            pytest.skip(f"Model loading test requires app setup: {e}")

class TestIntegration:
    """Integration tests for complete pipeline"""
    
    def test_end_to_end_prediction(self):
        """Test complete prediction pipeline"""
        try:
            # Create sample data
            sample_house = {
                'GrLivArea': 2000,
                'OverallQual': 7,
                'YearBuilt': 2005,
                'Neighborhood': 'NAmes',
                'GarageCars': 2
            }
            
            # This would test the complete pipeline if models exist
            # For now, just test data structure
            assert isinstance(sample_house, dict)
            assert all(key in sample_house for key in ['GrLivArea', 'OverallQual'])
            
        except Exception as e:
            pytest.skip(f"End-to-end test requires trained model: {e}")

class TestDataFiles:
    """Test data file integrity"""
    
    def test_data_files_exist(self):
        """Test that required data files exist"""
        data_dir = PROJECT_ROOT / "data"
        
        # Check if data directory exists
        if data_dir.exists():
            expected_files = ['train.csv']
            existing_files = [f.name for f in data_dir.glob('*.csv')]
            
            # At least one data file should exist
            assert len(existing_files) > 0, "No CSV files found in data directory"
        else:
            pytest.skip("Data directory not found")
    
    def test_model_directory(self):
        """Test model directory structure"""
        model_dir = PROJECT_ROOT / "model"
        
        if model_dir.exists():
            # Check for model files
            model_files = list(model_dir.glob('*.pkl'))
            # Don't require models to exist, just check structure
            assert model_dir.is_dir()
        else:
            pytest.skip("Model directory not found")

# Performance tests
class TestPerformance:
    """Performance and benchmark tests"""
    
    def test_prediction_speed(self):
        """Test prediction performance"""
        try:
            # Mock fast prediction test
            import time
            start_time = time.time()
            
            # Simulate prediction
            X_test = np.random.rand(100, 10)
            # prediction = mock_predict(X_test)
            
            end_time = time.time()
            prediction_time = end_time - start_time
            
            # Should be fast (under 1 second for 100 predictions)
            assert prediction_time < 1.0
            
        except Exception as e:
            pytest.skip(f"Performance test requires model: {e}")

if __name__ == "__main__":
    # Run tests when executed directly
    pytest.main([__file__, "-v", "--tb=short"])
